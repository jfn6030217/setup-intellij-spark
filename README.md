# setup-intellij-spark
Tutorial for setting up IntelliJ for Scala-Spark jobs

# Overview
By the end of this tutorial, you will have installed IntelliJ and 
launched a Spark job written in Scala.  

This tutorial is written for my Data Science friends who have
some experience with using Hadoop, and perhaps have even written
a few Python-Spark programs with PyCharm and run them with 
spark-submit, and want to write some Spark jobs in Scala.

There are four sections to this guide
1. Purchase and Download IntelliJ from JetBrains
2. Install and Configure IntelliJ for Scala/Spark Development
3. Create a trivial Scala/Park program and test it locally (Windows/MacOS)
4. Install a real Spark environment and deploy your program (Linux)


