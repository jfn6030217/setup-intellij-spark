# setup-intellij-spark
Tutorial for setting up IntelliJ for Scala-Spark jobs

# Overview
By the end of this tutorial, you will have installed IntelliJ and 
launched a Spark job written in Scala.  

This tutorial is written for my Data Science friends who have
some experience with using Hadoop, and perhaps have even written
a few Python-Spark programs with PyCharm and run them with 
spark-submit, and want to write some Spark jobs in Scala.

There are four sections to this guide
1. [Purchase and Download IntelliJ from JetBrains][1]
2. [Install and Configure IntelliJ for Scala/Spark Development][2]
3. [Create a trivial Scala/Park program and test it locally (Windows/MacOS)][3]
4. [Install a real Spark environment and deploy your program (Linux)][4]

[1]: section_1-download.md
[2]: section_2-install.md
[3]: section_3-test_locally.md
[4]: section_4-deploy_remotely.md

