# setup-intellij-spark
Tutorial for setting up IntelliJ for Scala-Spark jobs

# Overview
By the end of this tutorial, you will have installed IntelliJ and 
launched a Spark job written in Scala.  

"Everybody has to start somewhere"

Many Data Scientists have written Spark software in Python, using 
the PyCharm IDE and git.  However, this approach is text-based, so
it doesn't teach you how to compile and deploy compiled objects.

The first time you have to install and use an IDE with a compiler,
it helps to have someone introduce you to the important concepts
which may be new to you.  Usually this is taught person-to-person.

If you don't have someone to teach you, you will use google and 
youtube looking for tutorials.  I just wasn't happy with what I
found, so I wanted to create a complete end-to-end tutorial that 
did not assume you were already comfortable with IntelliJ, with 
the JVM, with compiling and deploying things.

This tutorial won't teach you what to do with Spark, nor how to
program in Scala, but will get you to the point where you can start
experimenting on your own.

There are four sections to this guide
1. [Purchase and Download IntelliJ from JetBrains][1]
2. [Install and Configure IntelliJ for Scala/Spark Development][2]
3. [Create a trivial Scala/Park program and test it locally (Windows/MacOS)][3]
4. [Install a real Spark environment and deploy your program (Linux)][4]

[1]: section_1-download.md
[2]: section_2-install.md
[3]: section_3-test_locally.md
[4]: section_4-deploy_remotely.md

